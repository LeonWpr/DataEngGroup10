{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1203491-fd11-414b-b4a6-e750bed5ae1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d05bf38-d498-45e3-8401-677c2899d834",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def classify_comments(df):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    def _classify(comment):\n",
    "        scores = sia.polarity_scores(comment)\n",
    "        compound = scores['compound']\n",
    "        if compound >= 0.05:\n",
    "            return 'positive'\n",
    "        elif compound <= -0.05:\n",
    "            return 'negative'\n",
    "        else:\n",
    "            return 'neutral'\n",
    "    df['sentiment'] = df['comment_column'].apply(lambda comment: _classify(comment))\n",
    "    return df\n",
    "\n",
    "def preprocess(df):\n",
    "    df['body'] = df['body'].apply(json.loads)\n",
    "    df['body'] = df['body'].apply(lambda x: x['body'])\n",
    "    df['body'] = df['body'].apply(lambda x: x.strip())\n",
    "    return df\n",
    "\n",
    "def analyze_comments(df):\n",
    "    df_counts = df['sentiment'].value_counts().reset_index()\n",
    "    df_counts.columns = ['sentiment', 'count']\n",
    "    df_counts = df_counts.sort_values(by='count', ascending=False)\n",
    "    return df_counts'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb295e5-2524-49f2-a155-e28f10a6560a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(rdd):\n",
    "    rdd = rdd.map(lambda line: json.loads(line))\\\n",
    "             .map(lambda line: line['body']) \\\n",
    "             .map(lambda line: line.strip())\n",
    "    return rdd\n",
    "\n",
    "\n",
    "def classify_comments(rdd):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "    def _classify(comment):\n",
    "        scores = sia.polarity_scores(comment)\n",
    "        compound = scores['compound']\n",
    "        if compound >= 0.05:\n",
    "            return 'positive'\n",
    "        elif compound <= -0.05:\n",
    "            return 'negative'\n",
    "        else:\n",
    "            return 'neutral'\n",
    "\n",
    "    m_rdd = rdd.map(lambda line: (line, _classify(line)))\n",
    "    return m_rdd\n",
    "\n",
    "\n",
    "def analyze_comments(rdd):\n",
    "    rdd = rdd.map(lambda pair: pair[1]) \\\n",
    "             .map(lambda k: (k, 1)) \\\n",
    "             .reduceByKey(lambda v1, v2: v1+v2)\\\n",
    "             .map(lambda kv: (kv[1], kv[0])) \\\n",
    "             .sortByKey(False)\\\n",
    "             .map(lambda vk: (vk[1], vk[0]))\n",
    "    return rdd\n",
    "\n",
    "\n",
    "def rdd_slice(rdd, start, end):\n",
    "    rdd = rdd.zipWithIndex()\\\n",
    "            .filter(lambda kv: kv[1] >= start and kv[1] <= end) \\\n",
    "            .map(lambda kv: kv[0])\n",
    "    return rdd     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b296924f-9bfd-4cd3-85dc-bd0afc0db26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter for the run\n",
    "# Data amounts to be testing\n",
    "chunk_amounts = [100, 300, 600]\n",
    "# Setting for 1,2 and 4 cores\n",
    "executor_cores_options = [1, 2]\n",
    "executor_nodes_options = [1, 2]\n",
    "# Tests per above setting\n",
    "iterations = 2\n",
    "\n",
    "spark_session = SparkSession.builder\\\n",
    "                           .appName(f\"DF_cores_{executor_cores}_instances_{executor_nodes}_datasize_{datasize*executor_nodes*executor_cores}\")\\\n",
    "                           .master(\"spark://192.168.2.230:7077\")\\\n",
    "                           .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "                           .config(\"spark.executor.instances\", executor_nodes)\\\n",
    "                           .config(\"spark.executor.cores\", executor_cores)\\\n",
    "                           .config(\"spark.dynamicAllocation.minExecutors\",\"1\")\\\n",
    "                           .config(\"spark.dynamicAllocation.maxExecutors\",\"1\")\\\n",
    "                           .config(\"spark.driver.port\", 9999)\\\n",
    "                           .config(\"spark.blockManager.port\", 10005)\\\n",
    "                           .getOrCreate()\n",
    "# Old API (RDD)\n",
    "spark_context = spark_session.sparkContext\n",
    "\n",
    "spark_context.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e01bf526-d9bb-410d-bebe-0c948c480954",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark_session' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m file_paths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhdfs://master:9000/path/in/hdfs/JsonFiles/corpus-webis-shortened_chunk\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3844\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark_session\u001b[49m\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mjson(file_paths)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark_session' is not defined"
     ]
    }
   ],
   "source": [
    "file_paths = [f\"hdfs://master:9000/path/in/hdfs/JsonFiles/corpus-webis-shortened_chunk{i}.json\" for i in range(1, 3844 + 1)]\n",
    "df = spark_session.read.option(\"multiline\", \"true\").json(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0696b5d0-a9ae-4bdf-909f-b207ee86e595",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
