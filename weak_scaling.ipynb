{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc17fb3-7f45-4c32-a9fc-1d5293361407",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################################################################################\n",
    "# In the weak scaling tests, the work for each core is kept constant while the total data is increase with the number of cores                  #\n",
    "# The code below does 12*iterations tests. there are 3 diffrent probelm sizes for the cluster and 4 diffrent configurations of the resources.   #\n",
    "# 1 worker 1 core                                                                                                                               #\n",
    "# 1 worker 2 cores                                                                                                                              #\n",
    "# 2 workers 1 core each                                                                                                                         #    \n",
    "# 2 worker 2 cores each                                                                                                                         #\n",
    "# Given the problem size the code test all the configurations iterations time                                                                   #\n",
    "# The data of the analysis is stored in results_df                                                                                              #\n",
    "#################################################################################################################################################\n",
    "\n",
    "# Imports\n",
    "!pip install pandas\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, split, array_contains\n",
    "import time\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "# Parameter for the run\n",
    "chunk_amounts = [100, 300, 600] # Number of files to be used in testing (each file is 100 comments)\n",
    "executor_nodes_options = [1, 2] # Number of workers\n",
    "executor_cores_options = [1, 2] # Number of cores per worker\n",
    "iterations = 2 # Tests per above setting\n",
    "\n",
    "# Downloading lexicon to be used for sentiment analysis\n",
    "nltk.download('vader_lexicon')\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    sentiment_score = sid.polarity_scores(text)\n",
    "    return sentiment_score['compound']\n",
    "\n",
    "sentiment_udf = udf(analyze_sentiment, FloatType())\n",
    "results = []\n",
    "\n",
    "#Test matrix. Tests every chossen number of chunks with every number of workers and cores per worker setting chosen\n",
    "for datasize in chunk_amounts:\n",
    "   for executor_nodes in executor_nodes_options:\n",
    "       for executor_cores in executor_cores_options:\n",
    "           #Sets up a sparksession for given test\n",
    "           spark_session = SparkSession.builder\\\n",
    "                           .appName(f\"DF_cores_{executor_cores}_instances_{executor_nodes}_datasize_{datasize*executor_nodes*executor_cores}\")\\\n",
    "                           .master(\"spark://192.168.2.230:7077\")\\\n",
    "                           .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "                           .config(\"spark.executor.instances\", executor_nodes)\\\n",
    "                           .config(\"spark.executor.cores\", executor_cores)\\\n",
    "                           .config(\"spark.dynamicAllocation.minExecutors\",\"1\")\\\n",
    "                           .config(\"spark.dynamicAllocation.maxExecutors\",\"1\")\\\n",
    "                           .config(\"spark.driver.port\", 9999)\\\n",
    "                           .config(\"spark.blockManager.port\", 10005)\\\n",
    "                           .getOrCreate()\n",
    "           #Loads in all the files\n",
    "           file_paths = [f\"hdfs://master:9000/path/in/hdfs/JsonFiles/corpus-webis-shortened_chunk{i}.json\" for i in range(1, datasize*executor_nodes*executor_cores + 1)]\n",
    "           df = spark_session.read.option(\"multiline\", \"true\").json(file_paths)\n",
    "\n",
    "           #Executing the tests \n",
    "           for iteration in range(iterations):\n",
    "                   start_time = time.time()\n",
    "                   df_with_sentiment = df.withColumn(\"sentiment_score\", sentiment_udf(df[\"content\"]))\n",
    "                   positive_comments = df_with_sentiment.filter(col(\"sentiment_score\") > 0).count()\n",
    "                   negative_comments = df_with_sentiment.filter(col(\"sentiment_score\") < 0).count()\n",
    "                   end_time = time.time()\n",
    "                   processing_time = end_time - start_time\n",
    "                   results.append({\n",
    "                       'Workers': executor_nodes,\n",
    "                       'Cores/W': executor_cores,\n",
    "                       'Total Cores': executor_cores*executor_nodes,\n",
    "                       'Chunks': datasize*executor_nodes*executor_cores,\n",
    "                       'Iteration': iteration + 1,\n",
    "                       'Time (s)': processing_time,\n",
    "                       'Positive Comments': positive_comments,\n",
    "                       'Negative Comments': negative_comments\n",
    "                       \n",
    "                   })\n",
    "           print(\"Session complete\")\n",
    "           spark_session.stop()\n",
    "           \n",
    "# Convert results to a pandas DataFrame for easy tabular display\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f79b9d9-8ec3-43c6-896a-51ddf751df29",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4367b379-da7b-44b5-90cd-5e71dfd5cb70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
