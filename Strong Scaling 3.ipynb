{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d864f0-6134-4012-b558-ff5486f09126",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import FloatType\n",
    "import time\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download the vader lexicon for sentiment analysis\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Initialize the Sentiment Intensity Analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    # Calculate the sentiment score of a text and return the compound score\n",
    "    sentiment_score = sid.polarity_scores(text)\n",
    "    return sentiment_score['compound']\n",
    "\n",
    "# Create a UDF for sentiment analysis\n",
    "sentiment_udf = udf(analyze_sentiment, FloatType())\n",
    "\n",
    "# Initialize results list and parameters for the simulation\n",
    "results = []\n",
    "chunk_amounts = [100, 600, 3600]  # Different sizes of data to process\n",
    "executor_cores_options = [1, 2]  # Number of cores per executor\n",
    "executor_nodes_options = [1, 2]  # Number of executor nodes\n",
    "iterations = 2  # Number of iterations per configuration\n",
    "\n",
    "for datasize in chunk_amounts:\n",
    "    # Generate HDFS file paths based on the data size\n",
    "    file_paths = [f\"hdfs://master:9000/path/in/hdfs/JsonFiles/corpus-webis-shortened_chunk{i}.json\" for i in range(1, datasize + 1)]\n",
    "    \n",
    "    for executor_nodes in executor_nodes_options:\n",
    "        for executor_cores in executor_cores_options:\n",
    "            # Initialize Spark session with specified configuration\n",
    "            spark_session = SparkSession.builder\\\n",
    "                           .appName(f\"STRONG_SCAL_DF_cores_{executor_cores}_instances_{executor_nodes}_datasize_{datasize}\")\\\n",
    "                           .master(\"spark://192.168.2.230:7077\")\\\n",
    "                           .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "                           .config(\"spark.executor.instances\", executor_nodes)\\\n",
    "                           .config(\"spark.executor.cores\", executor_cores)\\\n",
    "                           .config(\"spark.dynamicAllocation.minExecutors\",\"1\")\\\n",
    "                           .config(\"spark.dynamicAllocation.maxExecutors\",\"1\")\\\n",
    "                           .config(\"spark.driver.port\", 9999)\\\n",
    "                           .config(\"spark.blockManager.port\", 10005)\\\n",
    "                           .getOrCreate()\n",
    "\n",
    "            # Read and cache the data for performance\n",
    "            df = spark_session.read.option(\"multiline\", \"true\").json(file_paths).cache()\n",
    "            \n",
    "            # Apply sentiment analysis to the content column and cache the result\n",
    "            df_with_sentiment = df.withColumn(\"sentiment_score\", sentiment_udf(col(\"content\"))).cache()\n",
    "\n",
    "            for iteration in range(iterations):\n",
    "                start_time = time.time()\n",
    "                \n",
    "                # Count positive and negative coments using the cached dataframe:\n",
    "                positive_comments = df_with_sentiment.filter(col(\"sentiment_score\") > 0).count()\n",
    "                negative_comments = df_with_sentiment.filter(col(\"sentiment_score\") < 0).count()\n",
    "                \n",
    "                end_time = time.time()\n",
    "                processing_time = end_time - start_time\n",
    "                \n",
    "                # Collect the results for analysis\n",
    "                results.append({\n",
    "                    'Workers': executor_nodes,\n",
    "                    'Cores/W': executor_cores,\n",
    "                    'Total Cores': executor_cores * executor_nodes,\n",
    "                    'Chunks': datasize,\n",
    "                    'Iteration': iteration + 1,\n",
    "                    'Time (s)': processing_time,\n",
    "                    'Positive Comments': positive_comments,\n",
    "                    'Negative Comments': negative_comments\n",
    "                })\n",
    "            \n",
    "            # Cleanup and stop spark sesion\n",
    "            df.unpersist()\n",
    "            df_with_sentiment.unpersist()\n",
    "            spark_session.stop()\n",
    "\n",
    "# Convert results to a pandas DataFrame and print\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c2ab58-299b-413d-a510-42a888940b61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
